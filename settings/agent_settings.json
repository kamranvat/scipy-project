[
    {
        "name": "A2C-def",
        "active": false,
        "description": "A synchronous, deterministic variant of Asynchronous Advantage Actor Critic (A3C). It uses multiple workers to avoid the use of a replay buffer."
    },
    {
        "name": "A2C-opt",
        "active": false,
        "description": "A synchronous, deterministic variant of Asynchronous Advantage Actor Critic (A3C). It uses multiple workers to avoid the use of a replay buffer."
    },
    {
        "name": "DQN-def",
        "active": false,
        "description": "Deep Q Network (DQN) builds on Fitted Q-Iteration (FQI) and make use of different tricks to stabilize the learning with neural networks: it uses a replay buffer, a target network and gradient clipping."
    },
    {
        "name": "DQN-opt",
        "active": false,
        "description": "Deep Q Network (DQN) builds on Fitted Q-Iteration (FQI) and make use of different tricks to stabilize the learning with neural networks: it uses a replay buffer, a target network and gradient clipping."
    },
    {
        "name": "PPO-def",
        "active": false,
        "description": "The Proximal Policy Optimization algorithm combines ideas from A2C (having multiple workers) and TRPO (it uses a trust region to improve the actor)."
    },
    {
        "name": "PPO-opt",
        "active": true,
        "description": "The Proximal Policy Optimization algorithm combines ideas from A2C (having multiple workers) and TRPO (it uses a trust region to improve the actor). Note: the optimized hyperparameters actually call for 100k timesteps",
        "model_args": {
            "batch_size": 32,
            "clip_range": 0.2,
            "ent_coef": 0.0,
            "gae_lambda": 0.8,
            "gamma": 0.98,
            "learning_rate": 0.001,
            "n_epochs": 20,
            "n_steps": 256
        }
    }
]